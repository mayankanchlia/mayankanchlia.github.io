---
permalink: /projects/
title: "Projects"
excerpt: "Projects"
author_profile: true
redirect_from: 
  - /projects.html/
---

**Research/Industry projects**
* **Intelligent Human Pose Recommendation using Tensorflow**  
○ Leveraged skeleton data to ​ classify and recommend the best pose ​ possible based on the background scenes and objects  
○ Devised a ​ novel way​ for background analysis to get low-level features which could be used to recommend the best  
possible pose.  
○ Filed patent application in the US and India, ​ which is under review  
* **Camera Lite SDK**  
○ A lightweight SDK to provide Third-Party Camera Apps similar image quality as that of Samsung Camera App  
○ 3 billion captures ​ till date using the solution on a global scale  
○ High-level and low-level design and development of SDK  
* **Text to Video Emoji Content Generation using Tensorflow**  
○ An unsupervised recurrent neural network to find contextual and physical features from ​ articulated text  
○ Full-body tracking model to find body points and animate it according to temporal data from text. Got​ 88%
accuracy ​ compared to Text to Speech method.  
* **Google Camera X Extensions**  
○ Design and Implementation of the framework to provide developers to access Samsung specific image processing
solutions such as ​ HDR, night, Bokeh  
○ Contributed various interface designs which finally went to ​ AOSP​ . Implemented Vendor Extension for use case
realization.  
○ Co-worked with Google for a successful ​ [Google](https://www.youtube.com/watch?v=kuv8uK-5CLY) I/O and​ ​ [Samsung Developer Conference Demo](https://www.youtube.com/watch?v=S70mBGwMewY).  

**Course projects/Initiatives**  
* **Samsung Research Institute Bangalore**
○ Source Code Translation from Microsoft UWP to Tizen Platform
○ Designed and developed a toolkit to automatically translate the application code from one platform to another specifically Microsoft UWP to Tizen Platform
○ The work was based on Roslyn, XAML, C#
* **3D modelling and navigation in VR using Leap Motion Sensor.**
○ Created a more intuitive way to interact with a computer in VR using Leap Motion Sensor to track hands, and sensor data of a mobile phone to localize hands, track head and set the orientation of the player.
○ It detects Hand gestures to track inputs from the user and recognizes it. 
○ Used Unity game engine to design the application

* **Sign language and gesture recognition**
○ The objective of the project was to build a classifier to solve the following problems
A) Given an image of a static gesture, predict what alphabet it represents.
B) Given an image of a person's upper body and hand, locate the hand and classify the gesture. Since this project was a part of the ML course,
image processing operations like filtering, thresholding, color segmentation etc. were not allowed. We performed geometric operations like scaling,
rotating or cropping and conversion to gray-scale. We used the Scikit-Image library for this
 
For more details, please refer to my [resume]({{site.url}}/files/Resume.pdf) or contact me. 